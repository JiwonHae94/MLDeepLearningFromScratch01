{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPurpose of training a neural network is to minize the output of the loss function\\nby identifying the optimal parameters.\\nThe process of identifying the optimal parameters is called the 'optization'\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Purpose of training a neural network is to minize the output of the loss function\n",
    "by identifying the optimal parameters.\n",
    "The process of identifying the optimal parameters is called the 'optization'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SGD (stochamic gradient descent) is a \n",
    "optimization technique where optimal paramers is calculated by\n",
    "iteratively taking the derivative of the gradient\n",
    "\n",
    "W ← W - η * ∂L/∂W\n",
    "\n",
    "W : weight to be updated\n",
    "∂L/∂W : gradient of the loss fuinction of the W\n",
    "η : learning rate(constant. 0.01, 0.001 등)\n",
    "\n",
    "About of SGD \n",
    "\n",
    "SGD can easily be implmented, however it is ineffective depending on the situation\n",
    "for instance, for a function : \n",
    "\n",
    "f(x, y) = 1/20 * x² + y²\n",
    "\n",
    "gradient at each point is (x/10, 2y), and thereby steep on the y-axis, whereas it's not on the x-axis\n",
    "Also, minimum point(optimal) is (0, 0), when most of the points do not direct to the optimal point\n",
    "Hence, when SGD is applied to this formulae, it will converge in a zig-zag manner, in-arguably inefficient.\n",
    "\n",
    "In summary, SGD is ineffective for all anisotropy functions, which changes the gradient depending on the direction.\n",
    "\n",
    "Other optimization such as momentum, Adagrad or Adam were introduced to improve the problem that SGD has\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr= 0.01):\n",
    "        self.lr - lr\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6.1.4 Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Momentum\n",
    "v ← αv - η * ∂L/∂W\n",
    "W ← W + v\n",
    "\n",
    "W : weights to be updated\n",
    "∂L/∂W : gradient of the loss function of W\n",
    "η : learning rate\n",
    "v : velocity\n",
    "α : friction (0.9)\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "class Momentum :\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self. v = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            \n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nesterov:\n",
    "\n",
    "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
    "    # NAG는 모멘텀에서 한 단계 발전한 방법이다. (http://newsight.tistory.com/224)\n",
    "    \n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.v[key] *= self.momentum\n",
    "            self.v[key] -= self.lr * grads[key]\n",
    "            params[key] += self.momentum * self.momentum * self.v[key]\n",
    "            params[key] -= (1 + self.momentum) * self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5 AdaGrad : Learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When the learning rate η is set to be too small, it will delay the entire training phase,\n",
    "whereas if it's too big, the training will not yield appropriate result\n",
    "\n",
    "Learning rate decay is one of the approach to effectively decide learning rate. \n",
    "As the training progresses, it slowly reduces the learning rate. \n",
    "\n",
    "AdaGrad is an improved way of choosing the learning rate (from learning rate decay). \n",
    "It adaptively chooses the learning rate prior to implementation\n",
    "\n",
    "h ← h + ∂L/∂W ⊙ ∂L/∂W\n",
    "W ← W - η *1/√h * ∂L/∂W\n",
    "\n",
    "\n",
    "NOTE : as AdaGrad adds 2 square of the past gradient, \n",
    "the more the training progresses, it weakens. \n",
    "\n",
    "RMSProp was introduced to improve such problem. \n",
    "In RMSProp, gradient in the far past is slowly forgotten, and the new gradient is applied with high magnitude.\n",
    "It is call the \"exponential moving average\", where gradients are applied with weight in respect their \n",
    "timeline\n",
    "\"\"\"\n",
    "class AdaGrad: \n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            \n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSprop:\n",
    "\n",
    "    \"\"\"RMSprop\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adam is an optimization method which comings Momentum and AdaGrad\n",
    "It was introduced in 2015.\n",
    "\"\"\"\n",
    "class Adam :\n",
    "    def __init__(self, lr= 0.01, beta1 = 0.9, beta2 = 0.999) : \n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "                \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = {}\n",
    "            \n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        self.iter += 1 \n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2 ** self.iter) / (1.0 - self.beta1 ** self.iter)\n",
    "               \n",
    "        for key in params.keys():\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key] + 1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
